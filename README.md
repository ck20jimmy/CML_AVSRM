
# Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation

This is the official implementation of our paper "Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation", AAAI 2022 (Oral Presentation) 
\[[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/20210/19969)\]


Since I don't have enough bandwidth to clean up the code now, please refer to the implementation detail of this paper if you would like to reproduce it.

## Citation
```
@inproceedings{yang2022cross,
  title={Cross-modal mutual learning for audio-visual speech recognition and manipulation},
  author={Yang, Chih-Chun and Fan, Wan-Cyuan and Yang, Cheng-Fu and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={3},
  pages={3036--3044},
  year={2022}
}
```
